{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multinomial-Naive-Bayes-Disaster-Tweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arijqureshi/Disaster-Tweet-Prediction-Model/blob/main/Multinomial_Naive_Bayes_Disaster_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Necessary Packages"
      ],
      "metadata": {
        "id": "zty0yZqXfVJ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5Foa1VLAhPf"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as np\n",
        "import jax\n",
        "from jax import grad, jit, vmap\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os, shutil\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "vPqEVoAXmXt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating the Naive Bayes Class"
      ],
      "metadata": {
        "id": "bp5z7UjEffvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_string(str_arg):\n",
        "    # function to preprocess strings\n",
        "    # takes in a string\n",
        "    # removes everything that isn't a letter\n",
        "    # replaces multiple spaces with single spaces\n",
        "    # removes links\n",
        "    # removes emojis\n",
        "    # returns cleaned_str\n",
        "\n",
        "    cleaned_str = str_arg\n",
        "    cleaned_str=re.sub('[^a-z\\s]+',' ',cleaned_str,flags=re.IGNORECASE) #every char except alphabets is replaced\n",
        "    cleaned_str=re.sub('(\\s+)',' ',cleaned_str) #multiple spaces are replaced by single space\n",
        "    cleaned_str = re.sub(r'https?://\\S+', '', cleaned_str) # remove links\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    cleaned_str = emoji_pattern.sub(r'',cleaned_str) # remove emojis\n",
        "    cleaned_str=cleaned_str.lower() #converting the cleaned string to lower case\n",
        "\n",
        "    return cleaned_str # returning the preprocessed string\n",
        "\n",
        "def syn(word): # a function to find synonyms of words\n",
        "  synonyms = []\n",
        "  for syn in wordnet.synsets(word):\n",
        "    for lm in syn.lemmas():\n",
        "      synonyms.append(lm.name()) #adding into synonyms\n",
        "  if len(synonyms) != 0:\n",
        "    return synonyms[0] #return first synonym if it exists\n",
        "  else:\n",
        "    return word # else return word\n",
        "\n",
        "class NaiveBayes:\n",
        "    \n",
        "    def __init__(self,unique_classes, lsc, num_syn):\n",
        "        \n",
        "        self.classes=unique_classes # Constructor is sinply passed with unique number of classes of the training set\n",
        "        self.lsc = lsc\n",
        "        self.num_syn = num_syn\n",
        "\n",
        "    def addToBow(self,input,dict_index):\n",
        "        # a function to add each word to a dictionary for its corresponding class\n",
        "\n",
        "        if isinstance(input,numpy.ndarray): input=input[0] # to generalize for different formats of input\n",
        "        \n",
        "        noSyn = True\n",
        "        if self.num_syn != 0:\n",
        "          randnums = [random.randrange(len(input.split())) for _ in range(self.num_syn)]\n",
        "          noSyn = False\n",
        "        \n",
        "        i = 0\n",
        "        for token_word in input.split(): #for every word in preprocessed example\n",
        "            if not noSyn and i in randnums:\n",
        "              token_word = syn(token_word)\n",
        "            self.bow_dicts[dict_index][token_word]+=1 #increment in its count, will automatically add to dict if not already there\n",
        "            i += 1\n",
        "\n",
        "    def fit(self,dataset,labels):\n",
        "        \n",
        "        # the main function to train the MNNB model - computes a bag of words for each category/class\n",
        "\n",
        "    \n",
        "        self.examples=dataset\n",
        "        self.labels=labels\n",
        "        self.bow_dicts=numpy.array([defaultdict(lambda: 0) for index in range(self.classes.shape[0])])\n",
        "        \n",
        "        if not isinstance(self.examples,np.ndarray): self.examples=numpy.array(self.examples) # convert to numpy array if not already passed as one\n",
        "        if not isinstance(self.labels,np.ndarray): self.labels=numpy.array(self.labels)\n",
        "            \n",
        "        #constructing BoW for each category\n",
        "        for cat_ind, cat in enumerate(self.classes):\n",
        "          \n",
        "            all_cat_examples=self.examples[self.labels==cat] #filter all examples where target == cat\n",
        "            \n",
        "            # preprocess all examples\n",
        "            \n",
        "            cleaned_examples=[preprocess_string(cat_example) for cat_example in all_cat_examples]\n",
        "            \n",
        "            cleaned_examples=pd.DataFrame(data=cleaned_examples)\n",
        "            \n",
        "            #now costruct BoW of this particular category\n",
        "            numpy.apply_along_axis(self.addToBow,1,cleaned_examples,cat_ind)\n",
        "\n",
        "            # finished training model        \n",
        "      \n",
        "        \n",
        "      \n",
        "        # to speed up computation and not needlessly recalculate the same numbers over and over, we calculate\n",
        "        # prior probabilities of each class, our total vocabulary, and the denominator value for each class\n",
        "        # denom =  [ count(c) + |V| + 1 ] will be used at testing\n",
        "        # because testing formula: [ count(w|c) + lsc ] / [ count(c) + |V| + lsc ] } * p(c)\n",
        "\n",
        "        prob_classes=np.empty(self.classes.shape[0])\n",
        "        all_words=[]\n",
        "        cat_word_counts=np.empty(self.classes.shape[0])\n",
        "        for cat_ind,cat in enumerate(self.classes):\n",
        "           \n",
        "            #Calculating prior probability p(c) for each class\n",
        "            prob_classes = prob_classes.at[cat_ind].set(np.sum(self.labels==cat)/float(self.labels.shape[0]))\n",
        "            # prob_classes[cat_index]=np.sum(self.labels==cat)/float(self.labels.shape[0]) \n",
        "            \n",
        "            #Calculating total counts of all the words of each class \n",
        "            count=list(self.bow_dicts[cat_ind].values())\n",
        "            cat_word_counts = cat_word_counts.at[cat_ind].set(np.sum(np.array(list(self.bow_dicts[cat_ind].values())))+1)\n",
        "            # cat_word_counts[cat_index]=np.sum(np.array(list(self.bow_dicts[cat_index].values())))+1 # |v| is remaining to be added\n",
        "            \n",
        "            #get all words of this category                                \n",
        "            all_words+=self.bow_dicts[cat_ind].keys()\n",
        "                                                     \n",
        "        \n",
        "        #combine all unique words of every category to get vocabulary V of entire training set\n",
        "        \n",
        "        self.vocab=numpy.unique(numpy.array(all_words))\n",
        "        self.vocab_length=self.vocab.shape[0]\n",
        "                                  \n",
        "        #computing denominator value for each class                                     \n",
        "        denoms=np.array([cat_word_counts[cat_index]+self.vocab_length+self.lsc for cat_index,cat in enumerate(self.classes)])                                                                          \n",
        "      \n",
        "\n",
        "        # saving all information for each category in each element of cats_info for organization purposes\n",
        "        # each element of cats_info has a tuple that stores the dictionary at index 0, prior probability at index 1,\n",
        "        # and denom value at index 2\n",
        "        self.cats_info=[(self.bow_dicts[cat_index],prob_classes[cat_index],denoms[cat_index]) for cat_index,cat in enumerate(self.classes)]                               \n",
        "        self.cats_info=numpy.array(self.cats_info)                                 \n",
        "                                              \n",
        "                                              \n",
        "    def getExampleProb(self,test_example):                                \n",
        "                                            \n",
        "        # a function to get posterior probability for each class for given test_example\n",
        "                                        \n",
        "        likelihood_prob=np.zeros(self.classes.shape[0]) #to store probability w.r.t each class\n",
        "        #finding probability w.r.t each class of the given test example\n",
        "        for cat_ind,cat in enumerate(self.classes): \n",
        "\n",
        "            for test_token in test_example.split(): #split the test example and get p of each test word\n",
        "                                \n",
        "                # for each word w computes [ count(w|c)+lsc ] / [ count(c) + |V| + lsc ]                               \n",
        "                                              \n",
        "                #get total count of this test token from it's respective training dict to get numerator value                           \n",
        "                test_token_counts=self.cats_info[cat_ind][0].get(test_token,0)+self.lsc\n",
        "                \n",
        "                #now get likelihood of this test_token word by dividing by the denominator value we found earlier                          \n",
        "                test_token_prob=test_token_counts/float(self.cats_info[cat_ind][2])                              \n",
        "                \n",
        "\n",
        "                # take logs to prevent underflow\n",
        "                # we are able to just sum the logs since \n",
        "                # log(AB) = log(A) + log(B)\n",
        "                likelihood_prob = likelihood_prob.at[cat_ind].add(np.log(test_token_prob))\n",
        "                # likelihood_prob[cat_index]+=np.log(test_token_prob)\n",
        "                                              \n",
        "        # we have likelihood estimate of the given example against every class but we need posterior probility\n",
        "        post_prob=np.empty(self.classes.shape[0])\n",
        "        for cat_ind,cat in enumerate(self.classes):\n",
        "            # add likelihood estimate to prior probability to get posterior probability for each class\n",
        "            sum = likelihood_prob.at[cat_ind].get() + np.log(self.cats_info[cat_ind][1])\n",
        "            post_prob = post_prob.at[cat_ind].set(sum)\n",
        "            #post_prob[cat_index]=likelihood_prob[cat_index]+np.log(self.cats_info[cat_index][1])                                  \n",
        "      \n",
        "        return post_prob\n",
        "    \n",
        "   \n",
        "    def predict(self,test_set):    \n",
        "        # function to predict class values for array of given input\n",
        "        # returns predictions of the class values for each example in input\n",
        "\n",
        "        predictions=[] #to store prediction of each test example\n",
        "        for input in test_set: \n",
        "                                              \n",
        "            #preprocess the test example using the same function used for training set examples                                  \n",
        "            cleaned_input=preprocess_string(input) \n",
        "\n",
        "            #get prob of this example for both classes                          \n",
        "            posterior_prob=self.getExampleProb(cleaned_input) \n",
        "            \n",
        "            #pick the max value and store that in predictions array\n",
        "            predictions.append(self.classes[np.argmax(posterior_prob)])\n",
        "                \n",
        "        return np.array(predictions)\n"
      ],
      "metadata": {
        "id": "CS0J7gD5IKs_"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Data"
      ],
      "metadata": {
        "id": "nbDQw5IgHEuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle -v"
      ],
      "metadata": {
        "id": "QiaD0k-Wb8cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv ./kaggle.json /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "tunDaKe0BPVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle -v"
      ],
      "metadata": {
        "id": "RTH2oL6GBX5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls /root/.kaggle/"
      ],
      "metadata": {
        "id": "nAIpqv4WBb59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "6cPYp-EiBhFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions files -c nlp-getting-started"
      ],
      "metadata": {
        "id": "k9WwO6qZBkyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c nlp-getting-started"
      ],
      "metadata": {
        "id": "MDycw3LlBAgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil"
      ],
      "metadata": {
        "id": "9OkzmoVVF-Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('/content/disaster-tweets'):\n",
        "  os.mkdir('/content/disaster-tweets')"
      ],
      "metadata": {
        "id": "iLnOO-XCGAPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/nlp-getting-started.zip -d /content/disaster-tweets"
      ],
      "metadata": {
        "id": "TlnKjdABGKjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tweets = pd.read_csv('/content/disaster-tweets/train.csv')\n",
        "test_tweets = pd.read_csv('/content/disaster-tweets/test.csv')"
      ],
      "metadata": {
        "id": "kzXMSq3DGkse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing Hyperparamter Value Changes"
      ],
      "metadata": {
        "id": "sbUIqfPhQIQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_tweets['text'].values\n",
        "y = train_tweets['target'].values\n",
        "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=.2, random_state=2020)\n",
        "lsc_vals = np.zeros(10)\n",
        "test_accs = np.zeros(10)\n",
        "for i in range(0, 11):\n",
        "  lsc = float(i)/10\n",
        "  lsc_vals = lsc_vals.at[i].set(lsc)\n",
        "  nb = NaiveBayes(np.unique(y), lsc, 0) \n",
        "  nb.fit(X_train, y_train) # training the model\n",
        "  predClasses = nb.predict(X_test)\n",
        "  test_acc = np.sum(predClasses == y_test)/float(len(y_test))\n",
        "  test_accs = test_accs.at[i].set(test_acc)\n",
        "  print(lsc, test_acc)\n"
      ],
      "metadata": {
        "id": "NLOVWIqlFVBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lsc_vals, test_accs)\n",
        "plt.title(\"Model Accuracy vs. Hyperparameter Value\")\n",
        "plt.xlabel(\"Hyperparameter Value\")\n",
        "plt.ylabel(\"Model Test Accuracy\")"
      ],
      "metadata": {
        "id": "2mVoOYrGjXJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing Synonym Changes"
      ],
      "metadata": {
        "id": "WYANJK3mQa8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "syn_nums = np.zeros(9)\n",
        "test_accs = np.zeros(9)\n",
        "for i in range(1, 9):\n",
        "  syn_nums = syn_nums.at[i].set(i)\n",
        "  nb = NaiveBayes(np.unique(y), 1, i) \n",
        "  nb.fit(X_train, y_train) # training the model\n",
        "  predClasses = nb.predict(X_test)\n",
        "  test_acc = np.sum(predClasses == y_test)/float(len(y_test))\n",
        "  test_accs = test_accs.at[i].set(test_acc)\n",
        "  print(i, test_acc)"
      ],
      "metadata": {
        "id": "lT2TKpeLQdgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(syn_nums, test_accs)\n",
        "plt.title(\"Model Accuracy vs. Number of Synonyms Per Tweet\")\n",
        "plt.xlabel(\"Number of Synonyms\")\n",
        "plt.ylabel(\"Model Test Accuracy\")"
      ],
      "metadata": {
        "id": "KkDzk1NqQ3uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing Dataset Size"
      ],
      "metadata": {
        "id": "sQas2oTuRDVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len_data = np.zeros(9)\n",
        "test_accs = np.zeros(9)\n",
        "for i in range(1, 9):\n",
        "  split_size = float(i)/10 # calculating dataset split size (.1 means 10% for test)\n",
        "  X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=split_size, random_state=2020)\n",
        "  len_data = len_data.at[i].set(len(X_train))\n",
        "  nb = NaiveBayes(np.unique(y), 1, i) \n",
        "\n",
        "  nb.fit(X_train, y_train) # training the model\n",
        "  predClasses = nb.predict(X_test)\n",
        "\n",
        "  test_acc = np.sum(predClasses == y_test)/float(len(y_test)) # calculate test accuracy\n",
        "  test_accs = test_accs.at[i].set(test_acc)\n",
        "  print(len(X_train), test_acc)"
      ],
      "metadata": {
        "id": "I65tYJ3-RG-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(len_data, test_accs)\n",
        "plt.title(\"Model Accuracy vs. Train Dataset Size\")\n",
        "plt.xlabel(\"Train Dataset Size\")\n",
        "plt.ylabel(\"Model Test Accuracy\")"
      ],
      "metadata": {
        "id": "NUMXR-vySdXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimal Run of Model"
      ],
      "metadata": {
        "id": "OkcLRQNXS7zI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=.2, random_state=2020)\n",
        "nb = NaiveBayes(np.unique(y), 1, 5) \n",
        "nb.fit(X_train, y_train) # training the model\n",
        "predClasses = nb.predict(X_test)\n",
        "test_acc = np.sum(predClasses == y_test)/float(len(y_test))\n",
        "print(\"Final Test Accuracy:\", test_acc)"
      ],
      "metadata": {
        "id": "6a3xx1btS94a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}